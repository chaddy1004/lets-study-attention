{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MultiHead Attention\n",
    "\n",
    "By far the most important, and most significant component of the transformer is the Multihead Attention mechanism.\n",
    "After all, the paper is titled, \"Attention is all you need\". From now on, Multihead Attention will be shorthanded to MHA from not on for convenience. \n",
    "\n",
    "Therefore, before diving deep into the rest of the transformer, I think it is worthwhile to try implementing \n",
    "this module from scratch, which in turn will help us understand ths subtle parts about MHA.\n",
    "\n",
    "There does exist official PyTorch implementation, which is used for their official Transformer block, but I am a strong believer of\n",
    "trying something from scratch to appreciate the details when using a de-facto implementation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attention\n",
    "MHA is simply multiple Attention blocks stacked together, therefore in order to truly understand MHA, we need to do a deep dive on actual attention.\n",
    "From the paper, Attention is defined with this mathematical equation:\n",
    "$\\text{Attention}(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "with the following diagram to support the equation\n",
    "![Attention Diagram](breakdown/MultiheadAttention/images/attention.jpeg \"Attention Diagram from Paper\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q, K, and V\n",
    "Q, K, and V stand for Query, Key, and Value respectively. To cite this awesome [Stack Overfow Answer](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms)\n",
    "> The key/value/query concepts come from retrieval systems. For example, when you type a query to search for some video on Youtube, the search engine will map your query against a set of keys (video title, description etc.) associated with candidate videos in the database, then present you the best matched videos (values).\n",
    "\n",
    "One thing that really confused me personally when I was studying Attention is how they seemed to use the terms Q,K,V interchangeably even though they had different implications.\n",
    "The following is what I mean:\n",
    "![Attention Diagram 2](breakdown/MultiheadAttention/images/confusion.png \"Attention and MHA\")\n",
    "The input to both Scaled-Dot-Product Attention and Multi-Head Attention are Q,K,V, but Multi-Head Attention has Scaled-Dot-Product Attention in them, and it seems to take in Q,K,V that is projected with Linear Layer.\n",
    "Later on, they show the following equation for MHA that looks like the following:\n",
    "![MHA Equation](breakdown/MultiheadAttention/images/mha_equation.png \"MHA Equation\")\n",
    "\n",
    "Comparing it to the previos Attention equation which takes in Q,K,and V only, the Attention equation from MHA takes in \n",
    "$QW_{i}^{Q}$, $KW_{i}^{K}$, and $VW_{i}^{V}$. So which one is it...?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Maybe this was just me that was confused, but if you are like me who was also confused by this during the first read through of the paper, hopefully the following description may help.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, ModuleList\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}