{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MultiHead Attention\n",
    "\n",
    "By far the most important, and most significant component of the transformer is the Multihead Attention mechanism.\n",
    "After all, the paper is titled, \"Attention is all you need\". From now on, Multihead Attention will be shorthanded to MHA from not on for convenience. \n",
    "\n",
    "Therefore, before diving deep into the rest of the transformer, I think it is worthwhile to try implementing \n",
    "this module from scratch, which in turn will help us understand ths subtle parts about MHA.\n",
    "\n",
    "There does exist official PyTorch implementation, which is used for their official Transformer block, but I am a strong believer of\n",
    "trying something from scratch to appreciate the details when using a de-facto implementation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Attention\n",
    "MHA is simply multiple Attention blocks stacked together, therefore in order to truly understand MHA, we need to do a deep dive on actual attention.\n",
    "From the paper, Attention is defined with this mathematical equation:\n",
    "\n",
    "$\\text{Attention}(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "with the following diagram to support the equation\n",
    "![Attention Diagram](images/attention.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Q, K, and V\n",
    "Q, K, and V stand for Query, Key, and Value respectively. To cite this awesome [Stack Overfow Answer](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms)\n",
    "> The key/value/query concepts come from retrieval systems. For example, when you type a query to search for some video on Youtube, the search engine will map your query against a set of keys (video title, description etc.) associated with candidate videos in the database, then present you the best matched videos (values).\n",
    "\n",
    "One thing that really confused me personally when I was studying Attention is how they seemed to use the terms Q,K,V interchangeably even though they had different implications.\n",
    "The following is what I mean:\n",
    "![Attention Diagram 2](images/confusion.png \"Attention and MHA\")\n",
    "The input to both Scaled-Dot-Product Attention and Multi-Head Attention are Q,K,V, but Multi-Head Attention has Scaled-Dot-Product Attention in them, and it seems to take in Q,K,V that is projected with Linear Layer.\n",
    "Later on, they show the following equation for MHA that looks like the following:\n",
    "![MHA Equation](images/mha_equation.png \"MHA Equation\")\n",
    "\n",
    "Comparing it to the previos Attention equation which takes in Q,K,and V only, the Attention equation from MHA takes in \n",
    "$QW_{i}^{Q}$, $KW_{i}^{K}$, and $VW_{i}^{V}$. So which one is it...?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Maybe this was just me that was confused, but if you are like me who was also confused by this during the first read through of the paper, \n",
    "hopefully the following description may help.\n",
    "\n",
    "The image for Attention Module is correct in the sense that **Q** (Query), **K** (Key), **V** (Value) are the inputs, but I think that MHA diagram should have something else as its inputs instead of **V,K,Q**. These values should instead be some other input, and the vector that went through **Linear** layer should be **Q,K,V** that goes into each Attention head.\n",
    "I crudly redrew the Attention and MHA blocks to illustrate what I mean.\n",
    "![New Attention Diagram](images/AttentionDiagram_New.png \"New Attention Diagram for Illustration\")\n",
    "\n",
    "The Scaled Dot Product Attention box diagram would have the same internals as the previous diagram from the paper.\n",
    "\n",
    "The new equation that follows the new diagram would be: \n",
    "\n",
    "![MHA Equation](images/new_mha_equation.png \"MHA Equation\")\n",
    "\n",
    "$\\text{MultiHead}(Q,K,V)=\\text{Concat}(head_1,...,head_h)W^O$\n",
    "\n",
    "$\\text{where   } \\text{head_i} = \\text{Attention}(qW_i^Q, kvW_i^K, kvW_i^V)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **q** input gets turned into **Q** after being projected by $W^Q$, and **kv** gets turned into **K** and **V** after being projected by $W^K$ and $W^V$ respectively. **K** (Key), and **V** (Value) are extracted from the same inut vector. Personally, I am not sure why, but this is how it is in the paper. \n",
    "Instead of using the raw input $h$ times for each head of MHA, the authors decided to split up the input vector by projecting it with linear layer with output size of $d_k, d_k, d_v$ (dimensions for $q,k,v$ respectively) where $d_k = d_v = \\frac{d_{model}}{h} = 64$ \n",
    "(Not sure why the distinguished between $d_k$ and $d_v$, but in the paper, they are ultimately equal).\n",
    "This allowed for multiple heads to \"jointly attend to information from different representation subspaces at different positions\", which simply put,\n",
    "means the network is able to obtain information better by looking at the input differently through multiple heads.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In self-attention (used in Encoder portion), **q** and **kv** are also equal, where as with non-self-attention (used in Decoder portion), **q** and **kv** are different vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Attention Module\n",
    "\n",
    "Okay, so I think the general overview of the Attntion Module and MHA is somewhat complete. The rest of the detail will be understood better with code.\n",
    "\n",
    "The first part of creating MHA module is to create the Scaled Dot Product Attention Module. MHA is a simple stacking of multiple Attention Modules, so once the actual Attention Module is properly formed, we can expect the rest to be pretty straight forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import neccessary libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, ModuleList\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(Module):\n",
    "    def __init__(self, d_k, mask=None):\n",
    "        # initialize all the components here\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        self.mask = mask\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=self.d_k)\n",
    "        \n",
    "    \n",
    "    def forward(self, q,k,v):\n",
    "        \"\"\"\n",
    "        q,k,v are in lower case letters to match Python practice. It is same as the capital letter variables from above\n",
    "        All the vectors have dimension of (batch, seq_len, d_k)\n",
    "        :param q: Query \n",
    "        :param k: Key \n",
    "        :param v: Value\n",
    "        :return: Result vector with shape (batch, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        # need to transpose k to matrix multiply with q. Transposes row and col\n",
    "        k_T = torch.transpose(k, 1, -1) # k_T.shape = (batch, d_k, seq_len)\n",
    "        assert self.d_k == q.shape[-1] and self.d_k == k.shape[-1] and self.d_k == v.shape[-1], \"Dimension set and actual dimension does not match\" # shape = (batch, d_k)\n",
    "        # torch.matmul performs batch-matrix-multiplication.\n",
    "        # More detail on how matmul deals with its cases is listed here https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "        qkT = torch.matmul(q,k_T) # qkT.shape = (batch, seq_len, seq_len)\n",
    "        qkT_scaled = qkT * (self.d_k**(-0.5))\n",
    "        prob_map = None\n",
    "        if self.mask:\n",
    "            pass\n",
    "        else:\n",
    "            prob_map = F.softmax(input=qkT_scaled, dim=-1)\n",
    "        \n",
    "        # prob_map -> (batch, seq_len, seq_len)\n",
    "        # v => (batch, seq_len, d_k)\n",
    "        # Therefore, result -> (batch, seq_len, d_k), which is the same dimension as the original inputs\n",
    "        result = torch.matmul(prob_map, v)\n",
    "        return result \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Attention module is now created. For now, the mask is ignored, but we will get back to this when we deal with the decoder module.\n",
    "Notice that the output of the module has the same the same dimensions as the input vectors.\n",
    "\n",
    "Now that Attention Module is done, let's go ahead and make the MHA module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Coding MHA Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 64]) torch.Size([2, 5, 64]) torch.Size([2, 5, 64])\n",
      "torch.Size([2, 5, 64]) torch.Size([2, 5, 64]) torch.Size([2, 5, 64])\n",
      "torch.Size([2, 5, 64]) torch.Size([2, 5, 64]) torch.Size([2, 5, 64])\n",
      "torch.Size([2, 5, 64]) torch.Size([2, 5, 64]) torch.Size([2, 5, 64])\n",
      "torch.Size([2, 5, 64]) torch.Size([2, 5, 64]) torch.Size([2, 5, 64])\n",
      "torch.Size([2, 5, 64]) torch.Size([2, 5, 64]) torch.Size([2, 5, 64])\n",
      "torch.Size([2, 5, 64]) torch.Size([2, 5, 64]) torch.Size([2, 5, 64])\n",
      "torch.Size([2, 5, 64]) torch.Size([2, 5, 64]) torch.Size([2, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "wqs = ModuleList()\n",
    "wks = ModuleList()\n",
    "wvs = ModuleList()\n",
    "for head in range(8):\n",
    "    wqs.append(nn.Linear(in_features=512, out_features=64))\n",
    "    wks.append(nn.Linear(in_features=512, out_features=64))\n",
    "    wvs.append(nn.Linear(in_features=512, out_features=64))\n",
    "\n",
    "seq_len = 5\n",
    "inputs = torch.rand((2,seq_len, 512))\n",
    "for wq in wqs:\n",
    "    output = wq(inputs)\n",
    "    \n",
    "for wq,wv,wk in zip(wqs, wks, wvs):\n",
    "    output1 = wq(inputs)\n",
    "    output2 = wv(inputs)\n",
    "    output3 = wk(inputs)\n",
    "    print(output1.size(), output2.size(), output3.size())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Module):\n",
    "    def __init__(self, h, d_model, mask=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param h: number of heads  (8 in paper)\n",
    "        :param d_model: input feature size (512 in the paper)\n",
    "        :param mask: \n",
    "        \"\"\"\n",
    "        # initialize all the components here\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.mask = mask\n",
    "        self.h = h\n",
    "        self.d_k = self.d_model//self.h\n",
    "        self.attention = Attention(d_k=self.d_k, mask=self.mask)\n",
    "        \n",
    "        \n",
    "        self.wqs = ModuleList()\n",
    "        self.wks = ModuleList()\n",
    "        self.wvs = ModuleList()\n",
    "        \n",
    "        for head in range(self.h):\n",
    "            self.wqs.append(nn.Linear(in_features=d_model, out_features=self.d_k))\n",
    "            self.wks.append(nn.Linear(in_features=d_model, out_features=self.d_k))\n",
    "            self.wvs.append(nn.Linear(in_features=d_model, out_features=self.d_k))\n",
    "            \n",
    "        self.wo = nn.Linear(in_features=self.d_k*self.h, out_features=self.d_model)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        \"\"\"\n",
    "        q,k,v are in lower case letters to match Python practice. It is same as the capital letter variables from above\n",
    "        All the vectors have dimension of (batch, seq_len, d_k)\n",
    "        :param q: Input used to get \"q\" (query)\n",
    "        :param kv: Input used to get \"k\" (key) and \"v\" (value)\n",
    "        :return: Result vector with shape (batch, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        # recall that (seq_len, d_model)* (d_model, d_k) -> (seq_len, d_k) \n",
    "        # which is the matrix multiplication between input vector with linear layer (wq, wk, wv)\n",
    "        head_outputs = []\n",
    "        for wi_q, wi_k, wi_v in zip(self.wqs, self.wks, self.wvs):\n",
    "            Q = wi_q(q)\n",
    "            K = wi_k(kv)\n",
    "            V = wi_v(kv)\n",
    "            _output = self.attention(Q, K, V) # each output has size of (batch, seq_len, d_k)\n",
    "            head_outputs.append(_output)\n",
    "        attention_output = torch.cat([head_outputs], -1) # concat it to get (batch, seq_len, d_model)\n",
    "        output = self.wo(attention_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the major components needed to build the encoder portion of the Transfomer. \n",
    "The architecture of the Transformer can be viewed from this image, provided by the original paper.\n",
    "\n",
    "![Transformer Encoder Diagram](images/transformer_encoder.png \"Encoder of Transformer\")\n",
    "\n",
    "The normalization from \"Add & Norm\" block is Layer Normalization. \n",
    "\n",
    "Feed Forward according to paper is characterized by this equation:\n",
    "![Transformer Encoder Diagram](images/ffn_equation.png \"Encoder of Transformer\")\n",
    "\n",
    "$W_1$ and $W_2$ indicates that the feed forward network is made from two linear layers, with the input and output vector dimension being $d_in = d_out = 512$, and inner layer vector dimension of $d_{ff} = 2048$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer():\n",
    "    def __init__(self, h, d_model):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention()\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(in_features = 512, out_features = 2048), nn.Linear(in_features = 2048, out_features = 512))\n",
    "    def forward(self, x):\n",
    "        mha_output = self.mha(q=x, kv=x)\n",
    "        add_and_norm_1 = F.layer_norm(x+mha_output)\n",
    "        ffn_output = self.feed_forward(add_and_norm_1)\n",
    "        output = F.layer_norm(add_and_norm_1+ffn_output)\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
