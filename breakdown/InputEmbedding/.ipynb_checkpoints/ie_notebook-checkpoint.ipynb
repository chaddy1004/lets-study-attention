{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Input Encoding\n",
    "I think it is really important to go through the input encoding step, as it personally helps me\n",
    "understnad the network architecture way better if I know exactly how the input looks like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word's meaning represented as a vector\n",
    "\n",
    "For someone like me who was in vision this whole time and didn't really know too much about NLP, it might be worthwhile to read this part to get to know how words get embedded into a numerical input that could be used as an input to the network. \n",
    "\n",
    "Unlike, images where the image can directly be converted into a matrix with numerical values, the idea of \"converting\" word into vector seemed like a really foreign idea. \n",
    "\n",
    "### Embedding Layer - Look up table\n",
    "The simplest method of transforming word into vector is through the use of a look up table, where each word in the data is mapped to a unique vector in a look up table. These vectors are initialized randomly, and continually learns and changes during the training stage. This look up table and its initialization is already implemented in PyTorch's nn.Embedding(). \n",
    "\n",
    "Let's solidify the above concept by going through a very simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "### Setup\n",
    "Consider a toy dataset with the following two sentences:\n",
    "\n",
    "\\[**\"I like apples\"** **\"You like blueberries\"**\\]\n",
    "\n",
    "Each word can be transformed into a $d$-dimensional embedding vector. Let's use $d=4$\n",
    "\n",
    "For now, let us define each word with this indices:\n",
    "\n",
    "**I -> $0$**\n",
    "\n",
    "**Like -> $1$**\n",
    "\n",
    "**Apples -> $2$**\n",
    "\n",
    "**You -> $3$**\n",
    "\n",
    "**Blueberries -> $4$**\n",
    "\n",
    "\n",
    "\n",
    "Additionally, there are $5$ unique words in the dataset in total. Normally, this would be known through some preprocessing, but since this is a simple example, we can just deduce that information very simply.\n",
    "\n",
    "\n",
    "##### Note: \n",
    "For each words in the toy sentences used for this example, the case of the word is irrelevent. For example, **like** == **Like** == **LIKE** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Table\n",
    "The lookup table would be size $5$ by $3$ (row x col). Let's initialize PyTorch Embedding Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized LookUp Table by Pytorch's  Embedding Layer\n",
      "tensor([[[ 0.4894, -1.6609,  2.1959, -1.4890],\n",
      "         [ 0.2926,  0.3160,  0.2539,  0.9132],\n",
      "         [ 0.1907, -0.9287,  0.2203, -1.2850],\n",
      "         [ 1.6888, -0.9526, -1.4908, -0.3500],\n",
      "         [-0.4794, -0.5446,  0.4501,  1.9457]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import numpy as np\n",
    "\n",
    "# Initialize parameters\n",
    "num_words = 5\n",
    "embedding_dim = 4\n",
    "I = 0\n",
    "LIKE = 1\n",
    "APPLES = 2 \n",
    "YOU = 3 \n",
    "BLUEBERRIES = 4\n",
    "\n",
    "# Initialize Embedding Layer\n",
    "embedding_layer = nn.Embedding(num_embeddings = num_words, embedding_dim = embedding_dim)\n",
    "\n",
    "input_words = torch.LongTensor([[I, LIKE, APPLES, YOU, BLUEBERRIES]])\n",
    "\n",
    "lookup_table = embedding_layer(input_words)\n",
    "\n",
    "print(\"Initialized LookUp Table by Pytorch's  Embedding Layer\")\n",
    "print(lookup_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the above tensor represents the vector that each word in our dataset respresents. \n",
    "\n",
    "```\n",
    "input_words = torch.LongTensor([[I, LIKE, APPLES, YOU, BLUEBERRIES]])\n",
    "lookup_table = embedding_layer(input_words)\n",
    "```\n",
    "by putting **input_words** in **embedding_layer**, we are trying to retrieve the vector that each word represents.\n",
    "\n",
    "Therefore, the following is formed.\n",
    "```\n",
    "\"I\" = [0.4894, -1.6609,  2.1959, -1.4890]\n",
    "\"LIKE\" = [0.2926,  0.3160,  0.2539,  0.9132]\n",
    "\"APPLES\" = [0.1907, -0.9287,  0.2203, -1.2850],\n",
    "\"YOU\" = [1.6888, -0.9526, -1.4908, -0.3500]\n",
    "\"BLUEBERRIES\" = [-0.4794, -0.5446,  0.4501,  1.9457]\n",
    "```\n",
    "\n",
    "In layman's term we can say that the **meaning** of each word is represented by each vector.\n",
    "\n",
    "##### Important Note: \n",
    "By default, the Embedding layer is trainable, meaning the word vectors will slowly change during training. Words are not random in nature, so it makes sense that the word vectors that started off with random initialization will also shift to reflect the meaning and relationship between each words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word's position represented as a vector\n",
    "\n",
    "In natural language, the **position** of a word in a sentence is equally as important as its meaning. The authors of the paper came up with their own way of embedding the position of a word, which will get added to the \"meaning\" vector that was shown above. It is a bit complicated at first, so let's go through it slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Encoding Equation from paper\n",
    "The input encoding is done through the following method.\n",
    "![Encoding Equation](images/sin_and_cos.png \"sin and cos encoding\")\n",
    "\n",
    "In a sequential data such as sentences, the value (such as the individual word) matters of course, but the **position** of the value is equally as important.\n",
    "\n",
    "In order to embed this information, the authors decided to use the above equation to indiciate the position.\n",
    "\n",
    "Quoting the paper, \n",
    "> $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example \n",
    "Let's elaborate further by looking at each sentences from the toy dataset earlier.\n",
    "\n",
    "We will continue to use the same hyperparameters of **embedding_dim = 4** from earlier.\n",
    "\n",
    "**Note**: $d_{model} == \\text{embedding_dim}$ (I am using these two terms interchangebly)\n",
    "\n",
    "First consider the sentence, \n",
    "\n",
    "> I like apples\n",
    "\n",
    "Each word in the sentence will have its $pos$ value determined by its position in the sentence, where the word that is farthest to the left has $pos = 0$. Therefore,\n",
    "\n",
    "$\\text{I} \\rightarrow pos = 0$\n",
    "\n",
    "$\\text{like} \\rightarrow pos = 1$\n",
    "\n",
    "$\\text{apples} \\rightarrow pos = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the word \"I\" ($pos=0$), let us directly follow the Encoding Equation from earlier.\n",
    "\n",
    "$i$ from the equation would be enumertaed starting from $0$ to $\\text{int}(\\frac{\\text{embedding_dim}}{2})$\n",
    "\n",
    "### $i=0\\rightarrow 2i=0, 2i+1=1$\n",
    "\n",
    "$PE_{0,0} = sin(\\frac{0}{10000^{\\frac{0}{4}}}) = 0$\n",
    "\n",
    "$PE_{0,1} = cos(\\frac{0}{10000^{\\frac{0}{4}}}) = 1$\n",
    "\n",
    "### $i=1\\rightarrow 2i=2, 2i+1=3$\n",
    "\n",
    "$PE_{0,2} = sin(\\frac{0}{10000^{\\frac{2}{4}}}) = 0$\n",
    "\n",
    "$PE_{0,3} = cos(\\frac{0}{10000^{\\frac{2}{4}}}) = 1$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = torch.arange(5).reshape(1,1,-1)\n",
    "dim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = torch.arange(5).reshape(1,-1,1)\n",
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, 10).unsqueeze(1)\n",
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase = (pos/10000) ** (dim//5)\n",
    "phase.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Module):\n",
    "    def __init__(self, len_seq, d_model, dropout=0.1):\n",
    "        super(self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, len_seq, dtype=torch.float).unsqueeze(1) # shape = (len_seq, 1)\n",
    "        division_value =\n",
    "        pe = pe.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding():\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, len_seq, dtype=torch.float).unsqueeze  # shape = (len_seq, 1)\n",
    "    value = pos/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
