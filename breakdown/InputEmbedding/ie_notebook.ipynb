{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Input Encoding\n",
    "I think it is really important to go through the input encoding step, as it personally helps me\n",
    "understnad the network architecture way better if I know exactly how the input looks like.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "\n",
    "For someone like me who was in vision this whole time and didn't really know too much about NLP, it might be worthwhile to read this part to get to know how words get embedded into a numerical input that could be used as an input to the network. \n",
    "\n",
    "Unlike, images where the image can directly be converted into a matrix with numerical values, the idea of \"converting\" word into vector seemed like a really foreign idea. \n",
    "\n",
    "### Embedding Layer - Look up table\n",
    "The simplest method of transforming word into vector is through the use of a look up table, where each word in the data is mapped to a unique vector in a look up table. These vectors are initialized randomly, and continually learns and changes during the training stage. This look up table and its initialization is already implemented in PyTorch's nn.Embedding(). \n",
    "\n",
    "Let's solidify the above concept by going through a very simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "### Setup\n",
    "Consider a toy dataset with the following two sentences:\n",
    "\n",
    "\\[**\"I like apples\"** **\"You like blueberries\"**\\]\n",
    "\n",
    "Each word can be transformed into a $d$-dimensional embedding vector. Let's use $d=3$\n",
    "\n",
    "For now, let us define each word with this indices:\n",
    "\n",
    "**I -> $0$**\n",
    "\n",
    "**Like -> $1$**\n",
    "\n",
    "**Apples -> $2$**\n",
    "\n",
    "**You -> $3$**\n",
    "\n",
    "**Blueberries -> $4$**\n",
    "\n",
    "\n",
    "\n",
    "Additionally, there are $5$ unique words in the dataset in total. Normally, this would be known through some preprocessing, but since this is a simple example, we can just deduce that information very simply.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Table\n",
    "The lookup table would be size $5$ by $3$ (row x col). Let's initialize PyTorch Embedding Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized LookUp Table by Pytorch's  Embedding Layer\n",
      "tensor([[[ 0.6477,  1.1516, -0.7456],\n",
      "         [ 0.8228,  1.0397, -0.5882],\n",
      "         [-0.0581,  1.0294,  2.0208],\n",
      "         [-0.1690, -0.1230, -0.1516],\n",
      "         [ 0.2315, -2.6705,  0.1501]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import numpy as np\n",
    "\n",
    "# Initialize parameters\n",
    "num_words = 5\n",
    "embedding_dim = 3\n",
    "I = 0\n",
    "LIKE = 1\n",
    "APPLES = 2 \n",
    "YOU = 3 \n",
    "BLUEBERRIES = 4\n",
    "\n",
    "# Initialize Embedding Layer\n",
    "embedding_layer = nn.Embedding(num_embeddings = num_words, embedding_dim = embedding_dim)\n",
    "\n",
    "input_words = torch.LongTensor([[I, LIKE, APPLES, YOU, BLUEBERRIES]])\n",
    "\n",
    "lookup_table = embedding_layer(input_words)\n",
    "\n",
    "print(\"Initialized LookUp Table by Pytorch's  Embedding Layer\")\n",
    "print(lookup_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the above tensor represents the vector that each word in our dataset respresents. \n",
    "\n",
    "```\n",
    "input_words = torch.LongTensor([[I, LIKE, APPLES, YOU, BLUEBERRIES]])\n",
    "lookup_table = embedding_layer(input_words)\n",
    "```\n",
    "by putting **input_words** in **embedding_layer**, we are trying to retrieve the vector that each word represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "The input encoding is done through the following method.\n",
    "![Encoding Equation](images/sin_and_cos.png \"sin and cos encoding\")\n",
    "\n",
    "In a sequential data such as sentences, the value (such as the individual word) matters of course, but the **position** of the value is equally as important.\n",
    "\n",
    "In order to embed this information, the authors decided to use the above equation to indiciate the position.\n",
    "\n",
    "$i$ in the equation refers to the position that the value has in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = torch.arange(5).reshape(1,1,-1)\n",
    "dim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = torch.arange(5).reshape(1,-1,1)\n",
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, 10).unsqueeze(1)\n",
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase = (pos/10000) ** (dim//5)\n",
    "phase.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Module):\n",
    "    def __init__(self, len_seq, d_model, dropout=0.1):\n",
    "        super(self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, len_seq, dtype=torch.float).unsqueeze(1) # shape = (len_seq, 1)\n",
    "        division_value =\n",
    "        pe = pe.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding():\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, len_seq, dtype=torch.float).unsqueeze  # shape = (len_seq, 1)\n",
    "    value = pos/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
